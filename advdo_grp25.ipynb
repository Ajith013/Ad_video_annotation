{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Video Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a root directory with a suitable name and create a sub directory named script and add the following code the folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing the necessary libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import moviepy.editor as mp\n",
    "from pydub import AudioSegment\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import speech_recognition as sr\n",
    "import wx\n",
    "import re\n",
    "from imageai.Detection import VideoObjectDetection\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VERSIONS OF LIBRARIES USED\")\n",
    "print(\"Numpy : \" + np.__version__)\n",
    "print(\"OpenCV : \" + cv2.__version__)\n",
    "print(\"Pandas : \" + pd.__version__)\n",
    "print(\"PIL : \" + Image.__version__)\n",
    "print(\"Speech Recognition : \" + sr.__version__)\n",
    "print(\"wx : \" + wx.__version__)\n",
    "print(\"ibm watson : \" + ibm_watson.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Getting the root folder__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_path = os.path.abspath(os.getcwd())\n",
    "root_folder = os.path.abspath(os.path.join(script_path, os.pardir))\n",
    "print(\"The root folder is: \" + root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check whether the Video_ad folder is present in the root directory and if not create one and ask the user to paste the videos in that folder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Video_ad : Name of the folder with input videos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ad = root_folder + \"\\\\Video_ad\"\n",
    "\n",
    "if not os.path.exists(video_ad):\n",
    "    print(\"The folder Video_ad is created. Please paste the input videos in this folder\")\n",
    "    os.makedirs(video_ad)\n",
    "else:\n",
    "    print(\"The folder Video_ad already exists. Please paste the input videos in this folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating directories for extracted audio, extracted frames and processed frames__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create sub directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(sub_dir):\n",
    "    if not os.path.exists(sub_dir):\n",
    "        print(\"The folder {} is created.\".format(sub_dir))\n",
    "        os.makedirs(sub_dir)\n",
    "    else:\n",
    "        print(\"The folder {} already exists.\".format(sub_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_audio = root_folder + \"\\\\extracted_audio\"\n",
    "raw_audio = root_folder + \"\\\\raw_audio\"\n",
    "extracted_frames = root_folder + \"\\\\extracted_frames\"\n",
    "processed_frames = root_folder + \"\\\\processed_frames\"\n",
    "grayscale_frames = processed_frames + \"\\\\grayscale_frames\"\n",
    "scaled_frames = processed_frames + \"\\\\scaled_frames\"\n",
    "speech_text = root_folder + \"\\\\speech_text\"\n",
    "obj_detection = root_folder + \"\\\\object_detection\"\n",
    "\n",
    "create_dir(extracted_audio)\n",
    "create_dir(extracted_frames)\n",
    "create_dir(processed_frames)\n",
    "create_dir(grayscale_frames)\n",
    "create_dir(scaled_frames)\n",
    "create_dir(raw_audio)\n",
    "create_dir(speech_text)\n",
    "create_dir(obj_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check for videos in the input folder and cross check the format__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the video filename and video extension from the input video folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_ext(video_path):\n",
    "    filename = os.path.splitext(video_path)[0]\n",
    "    extension = os.path.splitext(video_path)[1]\n",
    "    return filename, extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add valid video path to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_video(video_dir):\n",
    "    video_path_list= []\n",
    "    \n",
    "    valid_video_extensions = [\".mp4\" , \"avi\"]\n",
    "    valid_vdo_extensions = [item.lower() for item in valid_video_extensions]\n",
    "    \n",
    "    for file in os.listdir(video_dir):\n",
    "        filename, extension = filename_ext(file)\n",
    "        if extension.lower() not in valid_vdo_extensions:\n",
    "            print(\"{} does not have a valid extension\".format(file))\n",
    "            continue\n",
    "        video_path_list.append((os.path.join(video_dir, file), filename))\n",
    "        print(filename)\n",
    "    return video_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_list = valid_video(video_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Images from the individual videos__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert frames to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scale the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_images(image):\n",
    "    h, w = image.shape\n",
    "    print('width:  ', w)\n",
    "    print('height: ', h)\n",
    "    height, width = image.shape\n",
    "    if height * width > 2120 * 1590:\n",
    "        image = cv2.resize(image, (1200, 1200))\n",
    "        print(image.shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(video_path, filename):\n",
    "    count = 0\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    success,image = video_capture.read()\n",
    "    success = True\n",
    "    w_base_filename = \"\"\n",
    "    image_suffix = '.jpg'\n",
    "    while success:\n",
    "        video_capture.set(cv2.CAP_PROP_POS_MSEC,(count*1000))\n",
    "        success,image = video_capture.read()\n",
    "        image_last = cv2.imread(\"frame{}.png\".format(count-1))\n",
    "        if np.array_equal(image,image_last):\n",
    "            break\n",
    "        \n",
    "        output_folder = extracted_frames + \"\\\\\" + filename\n",
    "        output_folder_grayscale = grayscale_frames + \"\\\\\" + filename\n",
    "        output_folder_scaled = scaled_frames + \"\\\\\" + filename\n",
    "        \n",
    "        create_dir(output_folder)\n",
    "        create_dir(output_folder_grayscale)\n",
    "        create_dir(output_folder_scaled)\n",
    "        \n",
    "        image_name = \"\\\\\"+ filename + \"frame\" + str(count) + image_suffix\n",
    "        grayscale_image_name = \"\\\\\"+ filename + \"gray_scale_frame\" + str(count) + image_suffix\n",
    "        scaled_image_name = \"\\\\\"+ filename + \"scaled_grayscale_frame\" + str(count) + image_suffix\n",
    "        \n",
    "        output = os.path.join(output_folder + image_name)\n",
    "        output_grayscale = os.path.join(output_folder_grayscale + grayscale_image_name)\n",
    "        output_scaled = os.path.join(output_folder_scaled + scaled_image_name)\n",
    "        \n",
    "        grayscale_image = grayscale(image)\n",
    "        scaled_image = scaled_images(grayscale_image)\n",
    "        \n",
    "        \n",
    "        cv2.imwrite(output, image)\n",
    "        cv2.imwrite(output_grayscale, grayscale_image)\n",
    "        cv2.imwrite(output_scaled, scaled_image)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract the audio from the video file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for normalization of peak volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_target_amplitude(sound, target_dBFS):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to end the audio rendering session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_clip(clip):\n",
    "    try:\n",
    "        clip.reader.close()\n",
    "        del clip.reader\n",
    "        if clip.audio != None:\n",
    "            clip.audio.reader.close_proc()\n",
    "            del clip.audio\n",
    "        del clip\n",
    "    except Exception as e:\n",
    "        sys.exc_clear() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract and write the audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_extraction(video_path, filename):\n",
    "    audio_suffix = '.wav'\n",
    "    \n",
    "    #Filename, folder and path to be saved\n",
    "    output_audio_filename = \"\\\\\" + filename + audio_suffix\n",
    "    output_audio_folder = raw_audio + \"\\\\\" + filename\n",
    "    create_dir(output_audio_folder)\n",
    "    output_file = os.path.join(output_audio_folder + output_audio_filename)\n",
    "    \n",
    "    \n",
    "    video_clip = mp.VideoFileClip(video_path)\n",
    "    video_clip.audio.write_audiofile(output_file)\n",
    "    close_clip(video_clip)\n",
    "    \n",
    "    audio = AudioSegment.from_file(output_file, \"wav\")\n",
    "    normalized_audio = match_target_amplitude(audio, -20.0)\n",
    "    channel_audio = normalized_audio.set_channels(1)\n",
    "    \n",
    "    processed_file = os.path.join(extracted_audio + output_audio_filename)\n",
    "    channel_audio.export(processed_file, format=\"wav\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extracting the frames and audio from the ad__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in video_path_list:\n",
    "    file_path = file[0]\n",
    "    filename = file[1]\n",
    "    \n",
    "    extract_images(file_path, filename)\n",
    "    audio_extraction(file_path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__OCR Implementation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find out valid images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_image(image_dir):\n",
    "    image_path_list= []\n",
    "    \n",
    "    valid_image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]\n",
    "    valid_img_extensions = [item.lower() for item in valid_image_extensions]\n",
    "    \n",
    "    for file in os.listdir(image_dir):\n",
    "        filename, extension = filename_ext(file)\n",
    "        if extension.lower() not in valid_img_extensions:\n",
    "            print(\"{} does not have a valid extension\".format(file))\n",
    "            continue\n",
    "        image_path_list.append((os.path.join(image_dir, file), filename))\n",
    "        print(filename)\n",
    "    return image_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to carryout OCR on the images extracted from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr(image_path):\n",
    "    pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    extracted_text = pytesseract.image_to_string(Image.open(image_path))\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funtion to calculate width and height of the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_cal(text, fontname, fontsize):\n",
    "    app = wx.App()\n",
    "    dc = wx.ScreenDC()\n",
    "    dc.SetFont(wx.Font(fontsize,wx.FONTFAMILY_DEFAULT, wx.FONTSTYLE_NORMAL, wx.FONTWEIGHT_BOLD, False, faceName=fontname))\n",
    "    dim = dc.GetTextExtent(text)\n",
    "    return dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory named OCRtext to store the csv files of text extracted from each video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocrtext = root_folder + \"\\\\OCRtext\"\n",
    "\n",
    "#Calling the function to create a directory\n",
    "create_dir(ocrtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to carry out OCR process and return the csv dictionary containing text for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_exec(image_path, filename):\n",
    "    extracted_text = ocr(image_path)\n",
    "    with open('temp_text.txt', 'w',5 ,'utf-8') as text_file:\n",
    "        text_file.write(extracted_text)\n",
    "    temp_file = open('temp_text.txt', 'r', encoding=\"utf8\") \n",
    "    Lines = temp_file.readlines()\n",
    "    CSV_dict = []\n",
    "    count = 1                \n",
    "    \n",
    "    \n",
    "    for line in Lines: \n",
    "        if re.match(r'^\\s*$', line):\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        text_size = dimension_cal(line, \"Calibri\", 11)\n",
    "        word_width= text_size[0]\n",
    "        word_height= text_size[1]\n",
    "        \n",
    "        data_dict = []\n",
    "\n",
    "        data_dict.append(count)\n",
    "        data_dict.append(filename)\n",
    "        data_dict.append(line)  # Text \n",
    "        data_dict.append(word_width)\n",
    "        data_dict.append(word_height)\n",
    "       \n",
    "        \n",
    "        CSV_dict.append(data_dict)\n",
    "        \n",
    "        if not CSV_dict:\n",
    "            continue\n",
    "            \n",
    "        count = count +1\n",
    "    \n",
    "    return CSV_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the processes and functions required for extracting text and storing it to the OCRtext directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(scaled_frames):\n",
    "    video_sort = os.path.join(scaled_frames, file)\n",
    "    \n",
    "    #Checking whether the image formats are valid using the function valid_image\n",
    "    image_path_list = valid_image(video_sort)\n",
    "    \n",
    "    #Creating folder for storing the csv file according to video names\n",
    "    video_folder_name = file\n",
    "    folder_path = ocrtext + \"\\\\\" + video_folder_name\n",
    "    create_dir(folder_path)\n",
    "    \n",
    "    #Going through each image in a scaled images folder of each video\n",
    "    for file in image_path_list:\n",
    "        file_path = file[0]\n",
    "        filename = file[1]\n",
    "        \n",
    "        csv_dict = ocr_exec(file_path, filename)\n",
    "        column_names = [\"Line_No\",\"File_name\" ,\"Text\",\"Word_width\" , \"Word_height\"]\n",
    "        ocr_df = pd.DataFrame.from_records(csv_dict, columns=column_names)\n",
    "        csv_name = folder_path + \"\\\\\" + filename + \"_ocr_text.csv\"\n",
    "        ocr_df.to_csv(csv_name, sep=',' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sp_txt(extracted_audio):\n",
    "    sp_txt = []\n",
    "    for file in os.listdir(extracted_audio):\n",
    "        path = os.path.join(extracted_audio, file)\n",
    "        r = sr.Recognizer()\n",
    "    \n",
    "        with sr.AudioFile(path) as source: # use \"test.wav\" as the audio source\n",
    "            audio = r.listen(source,True) # extract audio data from the file\n",
    "            try:\n",
    "                print(\"Transcribing\")\n",
    "                text = r.recognize_google(audio)\n",
    "            \n",
    "                info = []\n",
    "                info.append(path)\n",
    "                info.append(text)\n",
    "            \n",
    "                sp_txt.append(info)\n",
    "                print(text)\n",
    "            except:\n",
    "                print(\"Error\")\n",
    "    print (sp_txt)\n",
    "    return sp_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_txt_data = sp_txt(extracted_audio)\n",
    "column_names = [\"File_name\" ,\"Text\"]\n",
    "ocr_df = pd.DataFrame.from_records(sp_txt_data, columns=column_names)\n",
    "csv_name = speech_text + \"\\\\\" +  \"speech_text.csv\"\n",
    "ocr_df.to_csv(csv_name, sep=',' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = VideoObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath( os.path.join(execution_path , \"yolo.h5\"))\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_detection(input_path, output_path):\n",
    "    for file in os.listdir(input_path):\n",
    "        video_path = detector.detectObjectsFromVideo(input_file_path=os.path.join( input_path, file),\n",
    "                                output_file_path=os.path.join(output_path, file)\n",
    "                                , frames_per_second=29, log_progress=True)\n",
    "        print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_detection(video_ad, obj_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_tag = 'Energy lives here'\n",
    "extracted = ['Enamel is the strong white outer layer','this is electricity this is this is chamber ko thats me this is something is researching and external users to capita carbon emissions powerplay reducing CO2 emissions were also producing energy lives here', 'enamel is the strong wind or reliability with surface the thing is really important sentences is to make sure that the name of things strong and resilient for lifetime the more that we can strengthen and recorded in that used the word efficiency that they really want to recommend for strong and to strengthen and we had in the name of the has the investigation it can I give their patients the protection that they need and the virus']\n",
    "print(process.extract(given_tag, extracted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech To Text : IBM Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteTextFetchedFromFramesToCSV(audio_folder):\n",
    "    apikey = \"Jy87VR2jwgfVfD0iGh68YNbCgQBi3enVE9Hb_Nmh0VkD\"\n",
    "    url = \"https://api.eu-gb.speech-to-text.watson.cloud.ibm.com/instances/1882cc8e-4e5c-48a0-a63c-3658d1a88817\"\n",
    "    stt_folder = CreateFolderIfNotExists(os.path.join(audio_folder,\"SPEECHTEXT\"))\n",
    "    authenticator = IAMAuthenticator(apikey)\n",
    "    stt = SpeechToTextV1(authenticator=authenticator)\n",
    "    stt.set_service_url(url)\n",
    "    for file in os.listdir(audio_folder):\n",
    "        basefilename, file_extension = GetBaseFileNameAndExtension(file)\n",
    "        if(file_extension not in valid_audio_extensions):\n",
    "            continue\n",
    "        with open(os.path.join(audio_folder,file),'rb') as source:\n",
    "            res = stt.recognize(audio=source, content_type=\"audio/wav\", model=\"en-US_NarrowbandModel\", continuous=True).get_result()\n",
    "            CSV_data = list()\n",
    "            transcripted_text = list()\n",
    "            for i in range(len(res.get('results'))):\n",
    "                line_data = list()\n",
    "                transcript = res.get('results')[i].get('alternatives')[0].get('transcript')\n",
    "                transcripted_text.append(transcript)\n",
    "                confidence = res.get('results')[i].get('alternatives')[0].get('confidence')\n",
    "                line_data.append(i+1)\n",
    "                line_data.append(basefilename)\n",
    "                line_data.append(transcript)\n",
    "                line_data.append(confidence)\n",
    "                CSV_data.append(line_data)\n",
    "            print(process.extract(given_tag, transcripted_text))\n",
    "            if not CSV_data:\n",
    "                    continue\n",
    "            col_names = [\"LineNo\",\"FileName\" ,\"Transcript\",\"Confidence\"]\n",
    "            df_SpeechText = pd.DataFrame.from_records(CSV_data, columns=col_names)\n",
    "            df_SpeechText.to_csv(os.path.join(stt_folder,basefilename+\"_speechText.csv\"), sep=',' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WriteTextFetchedFromFramesToCSV(extracted_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection : YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov3(yolo_weights, yolo_cfg, coco_names):\n",
    "    net = cv2.dnn.readNet(yolo_weights, yolo_cfg)\n",
    "    classes = open(coco_names).read().strip().split(\"\\n\")\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return net, classes, output_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is used to perform object detection keeping threshold value 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_detection(net, img, output_layers, w, h, confidence_threshold):\n",
    "    blob = cv2.dnn.blobFromImage(img, 1 / 255., (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    for output in layer_outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Object is deemed to be detected\n",
    "            if confidence > confidence_threshold:\n",
    "               \n",
    "                center_x, center_y, width, height = list(map(int, detection[0:4] * [w, h, w, h]))\n",
    "               \n",
    "\n",
    "                top_left_x = int(center_x - (width / 2))\n",
    "                top_left_y = int(center_y - (height / 2))\n",
    "\n",
    "                boxes.append([top_left_x, top_left_y, width, height])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    return boxes, confidences, class_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is to draw the boxes aroud the objects detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, confidences, class_ids, classes, img, colors, confidence_threshold, NMS_threshold):\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, NMS_threshold)\n",
    "\n",
    "    FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            \n",
    "            color = colors[i]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "  \n",
    "            text = \"{}: {:.4f}\".format(classes[class_ids[i]], confidences[i])\n",
    "            cv2.putText(img, text, (x, y - 5), FONT, 0.5, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Detection\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_video_file(webcam, video_path, yolo_weights, yolo_cfg, coco_names, confidence_threshold, nms_threshold):\n",
    "    net, classes, output_layers = yolov3(yolo_weights, yolo_cfg, coco_names)\n",
    "    colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "    if webcam:\n",
    "        video = cv2.VideoCapture(0)\n",
    "        time.sleep(2.0)\n",
    "    else:\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while True:\n",
    "        ret, image = video.read()\n",
    "        h, w, _ = image.shape\n",
    "        boxes, confidences, class_ids = perform_detection(net, image, output_layers, w, h, confidence_threshold)\n",
    "        draw_boxes(boxes, confidences, class_ids, classes, image, colors, confidence_threshold, nms_threshold)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webcam = 0\n",
    "video_path = \"\"\n",
    "yolo_weights = \"yolov3.weights\"\n",
    "yolo_cfg = \"yolov3.cfg\"\n",
    "coco_names = \"coco_names.txt\"\n",
    "confidence_threshold = 0.5\n",
    "nms_threshold = 0.5\n",
    "\n",
    "for file in os.listdir(video_ad):\n",
    "    video_path = os.path.join(video_ad,file)\n",
    "    detection_video_file(webcam,video_path,yolo_weights,yolo_cfg,coco_names,confidence_threshold,nms_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
